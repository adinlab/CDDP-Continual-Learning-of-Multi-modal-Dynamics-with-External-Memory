{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "import argparse\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", default=\"CDDP\", choices=[\"CDDP\", \"VCL-BSSM\"])\n",
    "parser.add_argument(\"--task\", default=\"sinus\",\n",
    "                    choices=[\"sinus\", \"lv\", \"lorenz\"])\n",
    "parser.add_argument(\"--data_dir\", default=\"./data/\")\n",
    "parser.add_argument(\"--base\", default=\"runs\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=9)\n",
    "parser.add_argument(\"--start_replication\", type=int, default=1)\n",
    "parser.add_argument(\"--max_replication\", type=int, default=5)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "\n",
    "task = args.task\n",
    "if task == \"sinus\":\n",
    "    args.epochs = 300\n",
    "    args.latent_size = 6\n",
    "    args.eval_period = 60\n",
    "    args.lrate = 5e-3\n",
    "    args.context_size = 5\n",
    "    args.hidden_size = 40\n",
    "    args.n_samples = 10\n",
    "    args.dt = 0.1\n",
    "    args.memory_size = 20\n",
    "\n",
    "elif task == \"lv\":\n",
    "    args.epochs = 750\n",
    "    args.latent_size = 6\n",
    "    args.eval_period = 150\n",
    "    args.lrate = 1e-3\n",
    "    args.context_size = 8\n",
    "    args.hidden_size = 40\n",
    "    args.n_samples = 10\n",
    "    args.dt = 0.4\n",
    "    args.memory_size = 10\n",
    "\n",
    "elif task == \"lorenz\":\n",
    "    args.epochs = 500\n",
    "    args.latent_size = 12\n",
    "    args.eval_period = 100\n",
    "    args.lrate = 5e-4\n",
    "    args.context_size = 16\n",
    "    args.hidden_size = 90\n",
    "    args.n_samples = 10\n",
    "    args.dt = 0.01\n",
    "    args.memory_size = 15\n",
    "\n",
    "\n",
    "num_tasks_dict = {\n",
    "    \"sinus\": 5,\n",
    "    \"lv\": 4,\n",
    "    \"lorenz\": 4\n",
    "}\n",
    "\n",
    "sequences = {\n",
    "    'sinus': {\n",
    "        0: [4, 2, 3, 1, 0], 1: [0, 2, 3, 4, 1], 2: [2, 3, 0, 4, 1], 3: [0, 2, 1, 4, 3], \n",
    "        4: [3, 1, 2, 4, 0], 5: [3, 0, 2, 1, 4], 6: [0, 3, 2, 4, 1], 7: [2, 4, 3, 0, 1], \n",
    "        8: [1, 0, 3, 4, 2], 9: [4, 0, 2, 3, 1], 10: [1, 0, 4, 2, 3], 11: [3, 0, 2, 4, 1], \n",
    "        12: [1, 3, 2, 0, 4], 13: [2, 4, 3, 1, 0], 14: [1, 4, 3, 0, 2], 15: [2, 4, 1, 3, 0], \n",
    "        16: [0, 4, 2, 3, 1], 17: [1, 2, 4, 0, 3], 18: [1, 3, 2, 4, 0], 19:  [0, 3, 1, 2, 4]\n",
    "        },\n",
    "    'lv': {\n",
    "        0: [0, 1, 2, 3], 1: [0, 1, 3, 2], 2: [0, 2, 1, 3], 3: [0, 2, 3, 1], \n",
    "        4: [0, 3, 1, 2], 5: [0, 3, 2, 1], 6: [1, 0, 2, 3], 7: [1, 0, 3, 2], \n",
    "        8: [1, 2, 0, 3], 9: [1, 2, 3, 0], 10: [1, 3, 0, 2], 11: [1, 3, 2, 0], \n",
    "        12: [2, 0, 1, 3], 13: [2, 0, 3, 1], 14: [2, 1, 0, 3], 15: [2, 1, 3, 0], \n",
    "        16: [2, 3, 0, 1], 17: [2, 3, 1, 0], 18: [3, 0, 1, 2], 19: [3, 0, 2, 1]},\n",
    "    'lorenz': {\n",
    "        0: [0, 1, 2, 3], 1: [0, 1, 3, 2], 2: [0, 2, 1, 3], 3: [0, 2, 3, 1], \n",
    "        4: [0, 3, 1, 2], 5: [0, 3, 2, 1], 6: [1, 0, 2, 3], 7: [1, 0, 3, 2], \n",
    "        8: [1, 2, 0, 3], 9: [1, 2, 3, 0]},\n",
    "    }\n",
    "\n",
    "os.makedirs(f\"{args.base}\", exist_ok=True)\n",
    "os.makedirs(f\"{args.base}/baselines\", exist_ok=True)\n",
    "os.makedirs(f\"{args.base}/baselines/{args.task}\", exist_ok=True)\n",
    "os.makedirs(f\"{args.base}/baselines/{args.task}/{args.model}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatch:\n",
    "    def __init__(self, x, y=None, mode=None, t=None):\n",
    "        self.x = x  # high-dim data\n",
    "        self.y = y  # tasks\n",
    "        self.t = t  # times\n",
    "        self.mode = mode  # modes\n",
    "        self.N = x.shape[0]\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, xtr, ytr, modetr, ttr, xval=None, yval=None, xtest=None, ytest=None, modetest=None, ttest=None):\n",
    "        self.train = MyBatch(xtr, ytr, modetr, ttr)\n",
    "        if xtest is not None:\n",
    "            self.test = MyBatch(xtest, ytest, modetest, ttest)\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, Xtr, y=None, mode=None, t=None):\n",
    "        self.Xtr = Xtr \n",
    "        self.y = y\n",
    "        self.mode = mode\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Xtr)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xtr[idx], self.y[idx], self.mode[idx], self.t[idx]\n",
    "\n",
    "def load_dataset(task_name, data_dir, dt=0.001):\n",
    "    if task_name == \"sinus\":\n",
    "        name = \"sinus waves\"\n",
    "    elif task_name == \"lv\":\n",
    "        name = \"Lotka Volterra\"\n",
    "    elif task_name == \"lorenz\":\n",
    "        name = \"Lorenz attractor\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The {task} is not available\")\n",
    "        \n",
    "    Xtr = np.load(os.path.join(data_dir, name, \"training.npy\"))\n",
    "    Ytr = np.load(os.path.join(data_dir, name, \"training_tasks.npy\"))\n",
    "    Modetr = np.load(os.path.join(data_dir, name, \"training_modes.npy\"))\n",
    "    Ttr = dt * np.arange(0, Xtr.shape[1], dtype=np.float32)\n",
    "    Ttr = np.tile(Ttr, [Xtr.shape[0], 1])\n",
    "\n",
    "    Xtest = np.load(os.path.join(data_dir, name, \"test.npy\"))\n",
    "    Ytest = np.load(os.path.join(data_dir, name, \"test_tasks.npy\"))\n",
    "    Modetest = np.load(os.path.join(data_dir, name, \"test_modes.npy\"))\n",
    "    Ttest = dt * np.arange(0, Xtest.shape[1], dtype=np.float32)\n",
    "    Ttest = np.tile(Ttest, [Xtest.shape[0], 1])\n",
    "\n",
    "    dataset = MyDataset(Xtr,Ytr,Modetr, Ttr,xtest=Xtest,ytest=Ytest,modetest=Modetest, ttest=Ttest)\n",
    "    return dataset\n",
    "\n",
    "def load_data(task, data_dir, dt=0.1):\n",
    "    if task in [\"sinus\", \"lorenz\", \"lv\"]:\n",
    "        dataset = load_dataset(task, data_dir, dt)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The {task} is not available\")\n",
    "\n",
    "    [N, T, D] = dataset.train.x.shape\n",
    "    return dataset, N, T, D\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self, dataset, tasks, batch_size=8):\n",
    "        self.tasks = tasks\n",
    "        ids = dataset.y == tasks if isinstance(tasks, int) else np.array(list(map(lambda x: x in tasks, dataset.y[0])))\n",
    "        self.x = dataset.x[ids[0]] if isinstance(tasks, int) else dataset.x[ids]\n",
    "        self.y = dataset.y[ids] if isinstance(tasks, int) else dataset.y[:,ids][0]\n",
    "        self.modes = dataset.mode[ids] if isinstance(tasks, int) else dataset.mode[:,ids][0]\n",
    "        self.ts = dataset.t[ids[0]] if isinstance(tasks, int) else dataset.t[ids]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get(self):\n",
    "        ds = Dataset(self.x, self.y, self.modes, self.ts)\n",
    "        params = {'batch_size': self.batch_size, 'shuffle': True}\n",
    "\n",
    "        return data.DataLoader(ds, **params)\n",
    "\n",
    "\n",
    "\n",
    "def get_generators(task, dataset, data_dir, dt, batch_size, train_task, test_tasks):\n",
    "    if task in [\"sinus\", \"lorenz\", \"lv\"]:\n",
    "        gen_train = Generator(dataset=dataset.train, batch_size=batch_size, tasks=train_task).get()\n",
    "        gen_test = Generator(dataset=dataset.test, batch_size=batch_size, tasks=test_tasks).get()\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(f\"The {task} is not available\")\n",
    "    \n",
    "    gen_dict = {\n",
    "        \"train\": gen_train,\n",
    "        \"test\": gen_test\n",
    "    }\n",
    "\n",
    "    return gen_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_size, out_size, context_size, task, hidden_size=30, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_size = in_size\n",
    "        self.context_size = context_size\n",
    "        self.task = task\n",
    "        \n",
    "        self.activation = nn.Tanh\n",
    "\n",
    "        if self.task in [\"lorenz\"]:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(self.context_size * self.in_size, self.hidden_size),\n",
    "                self.activation(),\n",
    "                nn.LayerNorm(self.hidden_size),\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                self.activation(),\n",
    "                nn.LayerNorm(self.hidden_size),\n",
    "                nn.Linear(self.hidden_size, self.out_size * 2)\n",
    "            )\n",
    "        elif self.task in [\"sinus\", \"lv\"]:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(context_size * in_size, out_size * 2)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.__name__} is not implemented for {task}!\")\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x.flatten(1)) # embedding\n",
    "        return out[:, :self.out_size], out[:, self.out_size:].clamp(-8, 8).exp()\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_size, out_size, task, hidden_size=30, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.task = task\n",
    "\n",
    "        activation = nn.Tanh\n",
    "        \n",
    "        if self.task in [\"lorenz\"]:\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(in_size, hidden_size),\n",
    "                activation(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                activation(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, out_size)\n",
    "            )\n",
    "        elif self.task in [\"sinus\", \"lv\"]:\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.__name__} is not implemented for {task}!\")\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)                         \n",
    "\n",
    "class VBLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(VBLinear, self).__init__()\n",
    "        self.n_in = in_features\n",
    "        self.n_out = out_features\n",
    "\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.mu_w = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.prior_mu_w = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad=False)\n",
    "        self.prior_mu_w.data.zero_()\n",
    "\n",
    "        self.logsig2_w = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.prior_logsig2_w = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad=False)\n",
    "        self.prior_logsig2_w.data.zero_()\n",
    "        \n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.mu_w.size(1))\n",
    "        self.mu_w.data.normal_(0, stdv)\n",
    "        self.logsig2_w.data.zero_().normal_(-9, 0.001)  # var init via Louizos\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def KL(self):\n",
    "        return torch.distributions.kl.kl_divergence(Normal(self.mu_w, self.logsig2_w.clamp(-8, 8).exp().sqrt()), Normal(self.prior_mu_w, self.prior_logsig2_w.clamp(-8, 8).exp().sqrt()))\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        # Sampling free forward pass only if MAP prediction and no training rounds\n",
    "        if not self.training:\n",
    "            return torch.nn.functional.linear(input, self.mu_w, self.bias)\n",
    "        else:\n",
    "            mu_out = torch.nn.functional.linear(input, self.mu_w, self.bias)\n",
    "            logsig2_w = self.logsig2_w.clamp(-8, 8)\n",
    "            s2_w = logsig2_w.exp()\n",
    "            var_out = torch.nn.functional.linear(input.pow(2), s2_w) + 1e-8\n",
    "            return mu_out + var_out.sqrt() * torch.randn_like(mu_out)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__  + \" (\" + str(self.n_in) + \" -> \" + str(self.n_out)  + \")\"\n",
    "\n",
    "class F_theta(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_size, task, is_deterministic, reset):\n",
    "        super().__init__()\n",
    "        self.reset = reset\n",
    "        self.is_deterministic = is_deterministic\n",
    "\n",
    "        linear = nn.Linear if is_deterministic else VBLinear\n",
    "        activation = nn.Tanh\n",
    "        \n",
    "        if task in [\"sinus\", \"lorenz\", \"lv\"]:\n",
    "            self.F_theta = nn.Sequential(\n",
    "                linear(in_features=in_features, out_features=hidden_size),\n",
    "                activation(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                linear(in_features=hidden_size, out_features=out_features)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{self.__name__} is not implemented for {task}!\")\n",
    "\n",
    "        if reset:\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.F_theta(x)\n",
    "\n",
    "    def KL(self):\n",
    "        kl = 0\n",
    "        for layer in self.F_theta.children():\n",
    "            if isinstance(layer, VBLinear):\n",
    "                kl += layer.KL().sum()\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.F_theta.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif isinstance(layer, VBLinear):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "    def update_priors(self):\n",
    "        for layer in self.F_theta.children():\n",
    "            if isinstance(layer, VBLinear):\n",
    "                layer.prior_mu_w.data = layer.mu_w.data.clone()\n",
    "                layer.prior_logsig2_w.data = layer.logsig2_w.data.clone()\n",
    "                if self.reset:\n",
    "                    layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCL-BSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCL_BSSM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size, latent_size, task, context_size=1, hidden_size=30,\n",
    "                 device='cpu', n_samples=3, dt=0.1):\n",
    "        super(VCL_BSSM, self).__init__()\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_size = context_size\n",
    "        self.out_size = out_size\n",
    "        self.device = device\n",
    "        self.dt = dt\n",
    "        self.n_samples = n_samples\n",
    "        self.task = task\n",
    "\n",
    "        \n",
    "        self.mu0 = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=True)\n",
    "        self.logvar0 = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=True)\n",
    "\n",
    "        self.mu0_prior = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=False)\n",
    "        self.logvar0_prior = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=False)\n",
    "        \n",
    "        self.Sigma = torch.nn.Parameter(torch.ones(latent_size) * 0.1, requires_grad=False)\n",
    "\n",
    "        self.C = context_size\n",
    "        self.encoder = Encoder(in_size=in_size, context_size=context_size, out_size=latent_size, hidden_size=hidden_size, device=device, task=task)\n",
    "        self.decoder = Decoder(in_size=latent_size, out_size=out_size, task=task, hidden_size=hidden_size, device=device)\n",
    "\n",
    "        self.F_theta = F_theta(in_features=latent_size, hidden_size=hidden_size, out_features=latent_size, task=task, is_deterministic=False, reset=True)\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def prior_dynamics(self, x_prev):\n",
    "        mu = x_prev + self.F_theta(x_prev) * self.dt \n",
    "        var = (self.Sigma * self.dt).repeat(x_prev.shape[0], 1)\n",
    "        return mu, var\n",
    "\n",
    "    def forward(self, y):\n",
    "        batch_size, T, D = y.shape\n",
    "        lik = 0\n",
    "        x0_mu, x0_logvar = self.encoder(y[:, :self.context_size])\n",
    "\n",
    "        kl = torch.distributions.kl.kl_divergence(Normal(x0_mu, x0_logvar.sqrt()), Normal(self.mu0_prior, self.logvar0_prior.exp().sqrt()))\n",
    "\n",
    "        x_prev = x0_mu + torch.sqrt(x0_logvar)*Normal(0,1).sample()\n",
    "        \n",
    "        y_prob = torch.zeros_like(y, device=self.device)\n",
    "        for t in range(T):\n",
    "            mu, logvar = self.prior_dynamics(x_prev)\n",
    "            x_t = mu + torch.sqrt(logvar)*Normal(0,1).sample()\n",
    "            y_rcnst = self.decoder(x_t)\n",
    "            lik += - nn.MSELoss(reduction='sum')(y_rcnst, y[:, t,:]) \n",
    "\n",
    "            x_prev = x_t\n",
    "            y_prob[:, t] = y_rcnst\n",
    "\n",
    "        return lik.sum() - kl.sum() - self.F_theta.KL(), y_prob.cpu()\n",
    "        \n",
    "    def predict(self, y):\n",
    "        batch_size, T, D = y.shape\n",
    "        C = self.C\n",
    "        x0_mu, x0_logvar = self.encoder(y[:, :self.context_size])\n",
    "\n",
    "        x_prev = x0_mu + torch.sqrt(x0_logvar)*Normal(0,1).sample()\n",
    "        \n",
    "        y_prob = torch.zeros((batch_size, T - C, D), device=self.device)\n",
    "        for t in range(T):\n",
    "            mu, logvar = self.prior_dynamics(x_prev)\n",
    "            x_t = mu + torch.sqrt(logvar)*Normal(0,1).sample()\n",
    "            y_rcnst = self.decoder(x_t)\n",
    "            x_prev = x_t\n",
    "            if t - C >= 0:\n",
    "                y_prob[:, t-C] = y_rcnst\n",
    "\n",
    "        return y_prob.cpu()\n",
    "\n",
    "    def update(self):\n",
    "        self.F_theta.update_priors()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDDP(nn.Module):\n",
    "    def __init__(self, in_size, out_size, latent_size, task, context_size=1, hidden_size=30,\n",
    "                 device='cpu', n_samples=3, dt=0.1, memory_size=20):\n",
    "        super(CDDP, self).__init__()\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_size = context_size\n",
    "        self.out_size = out_size\n",
    "        self.device = device\n",
    "        self.dt = dt\n",
    "        self.n_samples = n_samples\n",
    "        self.task = task\n",
    "\n",
    "        \n",
    "        self.mu0 = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=True)\n",
    "        self.logvar0 = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=True)\n",
    "\n",
    "        self.mu0_prior = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=False)\n",
    "        self.logvar0_prior = torch.nn.Parameter(torch.zeros(latent_size), requires_grad=False)\n",
    "        \n",
    "        self.Sigma = torch.nn.Parameter(torch.ones(latent_size) * 0.1, requires_grad=False)\n",
    "\n",
    "        self.C = context_size\n",
    "        self.encoder = Encoder(in_size=in_size, context_size=context_size, out_size=latent_size, hidden_size=hidden_size, device=device, task=task)\n",
    "        self.decoder = Decoder(in_size=latent_size, out_size=out_size, task=task, hidden_size=hidden_size, device=device)\n",
    "\n",
    "        self.task = task\n",
    "        self.memory_size = memory_size\n",
    "        # memory\n",
    "        self.memory = torch.nn.Parameter(torch.Tensor(self.memory_size, self.latent_size), requires_grad=False)\n",
    "        self.memory.data.normal_(0, 0.01)\n",
    "        self.memory.data.pow_(2)\n",
    "        self.similarity = self.similarity_function\n",
    "        \n",
    "        self.alpha0 = torch.nn.Parameter(torch.ones(1, device=self.device), requires_grad=False)\n",
    "\n",
    "        self.v1 = nn.Linear(self.latent_size * 2, self.latent_size)\n",
    "        self.v2 = nn.Linear(self.latent_size * 2, self.latent_size)\n",
    "        \n",
    "        # transition model\n",
    "        self.F_theta = F_theta(in_features=latent_size*2, hidden_size=hidden_size, out_features=latent_size, task=task, is_deterministic=False, reset=False)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        return \n",
    "\n",
    "    def similarity_function(self, memory_sample, b):\n",
    "        # cosine sim\n",
    "        eps = 1e-8\n",
    "        a_n, b_n = memory_sample.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "        a_norm = memory_sample / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "        sim_mt = torch.mm(b_norm, a_norm.transpose(0, 1))\n",
    "        return sim_mt\n",
    "\n",
    "    def sample_from_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def update_memory(self, encoded_context):\n",
    "        # equation 12\n",
    "        memory_sample = self.sample_from_memory()\n",
    "        weights = self.get_memory_weights(encoded_context, memory_sample)\n",
    "        \n",
    "        mem_offset = torch.tanh((((1-weights).unsqueeze(2) * memory_sample) + (weights.unsqueeze(2) * encoded_context.unsqueeze(1))).mean(0))\n",
    "        self.memory.data.zero_().add_(mem_offset)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_memory_weights(self, encoded_context, memory_sample):\n",
    "        # equation 13\n",
    "        similarity_measure = self.similarity(memory_sample, encoded_context.squeeze())\n",
    "        weights = F.softmax(similarity_measure, dim=1)\n",
    "        return weights    \n",
    "\n",
    "    def prior_dynamics(self, x_prev, pi):\n",
    "        mu = x_prev + self.F_theta(torch.cat((self.memory[pi], x_prev), 1)) * self.dt \n",
    "        var = (self.Sigma * self.dt).repeat(x_prev.shape[0], 1)\n",
    "        return mu, var\n",
    "        \n",
    "    def get_memory_priors(self, batch_size):\n",
    "        K = self.memory_size\n",
    "        pi_primes = torch.distributions.Beta(torch.ones((K, batch_size), device=self.device), self.alpha0).sample()\n",
    "        prior_pis = torch.zeros((K, batch_size), device=self.device)\n",
    "        accumulator = torch.ones(batch_size, device=self.device)\n",
    "        for k in range(K):\n",
    "            prior_pi_k = pi_primes[k]\n",
    "            j = k -1\n",
    "            if j >= 0:\n",
    "                accumulator = accumulator * (1 - pi_primes[j])\n",
    "            prior_pi_k = prior_pi_k * accumulator\n",
    "            prior_pis[k] = prior_pi_k\n",
    "        \n",
    "        return Categorical(prior_pis.transpose(1,0))\n",
    "\n",
    "    def forward(self, y):\n",
    "        batch_size, T, D = y.shape\n",
    "        lik = 0\n",
    "        encoder_mu, encoder_logvar = self.encoder(y[:, :self.context_size])\n",
    "        weights = self.update_memory(encoded_context=encoder_mu)\n",
    "        \n",
    "        # equation 29 \n",
    "        posterior_pi = Categorical(weights)\n",
    "        prior_pi = self.get_memory_priors(batch_size=y.shape[0])\n",
    "        kl_pi = torch.distributions.kl.kl_divergence(posterior_pi, prior_pi)\n",
    "        pi = posterior_pi.sample()\n",
    "\n",
    "        # equation 30\n",
    "        x0_mu = self.v1(torch.cat((self.memory[pi], encoder_mu.squeeze()), 1))\n",
    "        x0_logvar = self.v2(torch.cat((self.memory[pi], encoder_mu.squeeze()), 1)).clamp(-8, 8).exp()\n",
    "        \n",
    "\n",
    "        kl = torch.distributions.kl.kl_divergence(Normal(x0_mu, x0_logvar.sqrt()), Normal(self.mu0_prior, self.logvar0_prior.exp().sqrt()))\n",
    "\n",
    "        x_prev = x0_mu + torch.sqrt(x0_logvar)*Normal(0,1).sample()\n",
    "        \n",
    "        y_prob = torch.zeros_like(y, device=self.device)\n",
    "        for t in range(T):\n",
    "            mu, logvar = self.prior_dynamics(x_prev, pi)\n",
    "            x_t = mu + torch.sqrt(logvar)*Normal(0,1).sample()\n",
    "            y_rcnst = self.decoder(x_t)\n",
    "            lik += - nn.MSELoss(reduction='sum')(y_rcnst, y[:, t,:]) \n",
    "            \n",
    "            x_prev = x_t\n",
    "            y_prob[:, t] = y_rcnst\n",
    "\n",
    "        return lik.sum() - (kl.sum() + self.F_theta.KL() + kl_pi.sum()), y_prob.cpu()\n",
    "\n",
    "    def predict(self, y):      \n",
    "        batch_size, T, D = y.shape\n",
    "        C = self.C\n",
    "        encoder_mu, encoder_logvar = self.encoder(y[:, :self.context_size])\n",
    "        memory_sample = self.sample_from_memory()\n",
    "        weights = self.get_memory_weights(encoder_mu, memory_sample)\n",
    "        pi = Categorical(weights).sample()\n",
    "        x0_mu = self.v1(torch.cat((self.memory[pi], encoder_mu), 1))\n",
    "        x0_logvar = self.v2(torch.cat((self.memory[pi], encoder_mu), 1)).clamp(-8, 8).exp()\n",
    "\n",
    "        x_prev = x0_mu + torch.sqrt(x0_logvar)*Normal(0,1).sample()\n",
    "        \n",
    "        y_prob = torch.zeros((batch_size, T - C, D), device=self.device)\n",
    "        for t in range(T):\n",
    "            mu, logvar = self.prior_dynamics(x_prev, pi)\n",
    "            x_t = mu + torch.sqrt(logvar)*Normal(0,1).sample()\n",
    "            y_rcnst = self.decoder(x_t)\n",
    "            x_prev = x_t\n",
    "            if t - C >= 0:\n",
    "                y_prob[:, t-C] = y_rcnst\n",
    "\n",
    "        return y_prob.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMSE_score_train(preds, targets):\n",
    "    return (torch.square(targets - preds).mean() / torch.square(targets).mean()).item()\n",
    "\n",
    "def NMSE_score(preds, targets):\n",
    "    return (torch.square(targets - preds.mean(0)).mean() / torch.square(targets).mean()).item()\n",
    "\n",
    "def normal(target, mean):\n",
    "    sigma = 0.1\n",
    "    return 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(-0.5*np.square((target - mean)/sigma))\n",
    "\n",
    "def NLL_score(preds, targets):\n",
    "    N_sample, batch_size, T, D = preds.shape\n",
    "    sum_of_normals = np.zeros((batch_size, D))\n",
    "    for t in range(T):\n",
    "        sum_of_normals_sample = np.zeros((batch_size, D))\n",
    "        for sample in range(N_sample):\n",
    "            pred_point = preds[sample, :, t, :]\n",
    "            target_point = targets[:, t, :]\n",
    "            tmp = normal(target_point, pred_point)\n",
    "            sum_of_normals_sample = np.add(sum_of_normals_sample, tmp)\n",
    "        sum_of_normals_sample = sum_of_normals_sample / N_sample\n",
    "        sum_of_normals = np.add(sum_of_normals, sum_of_normals_sample)\n",
    "    mean_of_normals = sum_of_normals / T\n",
    "    nll = - np.log(mean_of_normals + 1e-3)\n",
    "    return nll.mean().item()\n",
    "\n",
    "\n",
    "def calculate_scores(probs, targets):\n",
    "    return (\n",
    "        NMSE_score(probs, targets), \n",
    "        NLL_score(probs, targets)\n",
    "    )\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n",
    "def train(model, optim, dataloader, task, device='cpu'):\n",
    "    optim.zero_grad()\n",
    "    model.train()\n",
    "    train_elbo = 0\n",
    "    train_num = 0\n",
    "    y_probs = []\n",
    "    targets = []\n",
    "    for local_batch in dataloader:\n",
    "        y, task_, mode, ts = local_batch\n",
    "        y = FloatTensor(y.float()).to(device)\n",
    "        elbo, y_prob = model(y)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_elbo += elbo.item()\n",
    "        train_num += y.shape[0]\n",
    "\n",
    "        y_probs.append(y_prob)\n",
    "        targets.append(y.cpu())\n",
    "\n",
    "    elbo = train_elbo / train_num\n",
    "    nmse = NMSE_score_train(torch.vstack(y_probs), torch.vstack(targets))\n",
    "    return elbo, nmse\n",
    "\n",
    "def eval(task_name, model, dataloader, train_task, test_tasks, n_samples, name, device=\"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        targets = []\n",
    "        y_probs = [[] for _ in range(n_samples)]\n",
    "        tasks = []\n",
    "        modes = []\n",
    "        for local_batch in dataloader:\n",
    "            y, task, mode, ts = local_batch\n",
    "            y = FloatTensor(y.float()).to(device)\n",
    "            for sample in range(n_samples):\n",
    "                y_prob = model.predict(y)\n",
    "                y_probs[sample].extend(y_prob)\n",
    "            \n",
    "            targets.append(y[:, model.C:, :])\n",
    "            tasks.append(task)\n",
    "            modes.append(mode)\n",
    "        \n",
    "        y_probs = [torch.stack(item) for item in y_probs]\n",
    "        y_probs, targets = torch.stack(y_probs).cpu(), torch.vstack(targets).cpu()\n",
    "\n",
    "        tasks = torch.cat(tasks)\n",
    "        modes = torch.cat(modes)\n",
    "        nmse_mean, nll_mean = calculate_scores(y_probs, targets)\n",
    "        \n",
    "\n",
    "    return nmse_mean, nll_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(args, device, experiment_num):\n",
    "    name = f\"{args.base}/baselines/{args.task}/{args.model}/experiment_{str(experiment_num).zfill(2)}\"\n",
    "    os.makedirs(name, exist_ok=True)\n",
    "    \n",
    "    num_tasks = num_tasks_dict[args.task]\n",
    "    tasks = sequences[args.task].copy()[experiment_num - 1].copy()\n",
    "    train_task = tasks.pop()\n",
    "    test_tasks = []\n",
    "    test_tasks.append(train_task)\n",
    "    print(f\"train task: {train_task}, test tasks: {test_tasks}\")\n",
    "\n",
    "    dataset, N, T, D = load_data(args.task, args.data_dir, dt=args.dt)\n",
    "    gen_dict = get_generators(args.task, dataset, args.data_dir, args.dt, args.batch_size, train_task, test_tasks)\n",
    "    n_epochs = args.epochs * num_tasks\n",
    "\n",
    "    context_size = args.context_size\n",
    "    in_size = D\n",
    "    out_size = D\n",
    "    latent_size = args.latent_size\n",
    "    hidden_size = args.hidden_size\n",
    "\n",
    "    if args.model == \"VCL-BSSM\":\n",
    "        model = VCL_BSSM(in_size=in_size, out_size=out_size, latent_size=latent_size, task=args.task,  context_size=context_size,\n",
    "                                hidden_size=hidden_size, device=device, n_samples=args.n_samples, dt=args.dt)\n",
    "    elif args.model == \"CDDP\":\n",
    "        model = CDDP(in_size=in_size, out_size=out_size, latent_size=latent_size, task=args.task,  context_size=context_size,\n",
    "                    hidden_size=hidden_size, device=device, n_samples=args.n_samples, dt=args.dt, memory_size=args.memory_size)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The {args.model} is not available\")\n",
    "        \n",
    "    model.to(device)\n",
    "\n",
    "    lrate = args.lrate\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "    history = {\"elbo\": [], \"nmse_train\": [], \"nmse_test\": [], \"nll_test\": [], \"idx_train\": [], \"idx_test\": []}\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        elbo, score = train(model, optim, gen_dict[\"train\"], args.task, device)\n",
    "\n",
    "        history[\"elbo\"].append(elbo)\n",
    "        history[\"nmse_train\"].append(score)\n",
    "        history[\"idx_train\"].append(epoch)\n",
    "\n",
    "        if epoch % args.eval_period == 0 or epoch == n_epochs - 1:\n",
    "            mse, nll = eval(args.task, model, gen_dict[\"test\"], train_task, test_tasks, args.n_samples, f\"{name}/images/{epoch}.png\",device)\n",
    "            history[\"nmse_test\"].append(mse)\n",
    "            history[\"nll_test\"].append(nll)\n",
    "            history[\"idx_test\"].append(epoch)\n",
    "            print(f\"Epoch {epoch}/{n_epochs} || Train: Elbo: {history['elbo'][-1]:.4f} NMSE: {history['nmse_train'][-1]:.5f}, Test NMSE: {history['nmse_test'][-1]:.5f}, NLL: {history['nll_test'][-1]:.5f}\")\n",
    "            \n",
    "        \n",
    "        if ((epoch % (n_epochs // num_tasks)) == 0 and epoch != 0) or epoch == n_epochs -1:\n",
    "            state = {\n",
    "                \"model_state_dict\": model.state_dict()\n",
    "            }\n",
    "            torch.save(state, f\"{name}/checkpoint_trained_on_task_{train_task}.pt\")\n",
    "\n",
    "            if epoch == n_epochs -1:\n",
    "                continue\n",
    "                \n",
    "            model.update()\n",
    "            \n",
    "            train_task = tasks.pop()\n",
    "            test_tasks.append(train_task)\n",
    "            print(f\"train task: {train_task}, test tasks: {test_tasks}\")\n",
    "\n",
    "            gen_dict = get_generators(args.task, dataset, args.data_dir, args.dt, args.batch_size, train_task, test_tasks)\n",
    "    \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 1 ##########\n",
      "train task: 0, test tasks: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d98bf5b961e41cb9c49b27debd040b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1500 || Train: Elbo: -359.5756 NMSE: 1.03453, Test NMSE: 0.99228, NLL: 2.96502\n",
      "Epoch 60/1500 || Train: Elbo: -354.5408 NMSE: 0.96687, Test NMSE: 1.02593, NLL: 3.83257\n",
      "Epoch 120/1500 || Train: Elbo: -320.3667 NMSE: 0.42563, Test NMSE: 0.55307, NLL: 2.67718\n",
      "Epoch 180/1500 || Train: Elbo: -330.2021 NMSE: 0.56372, Test NMSE: 0.43563, NLL: 2.36066\n",
      "Epoch 240/1500 || Train: Elbo: -315.5135 NMSE: 0.33073, Test NMSE: 0.41808, NLL: 1.77968\n",
      "Epoch 300/1500 || Train: Elbo: -312.1187 NMSE: 0.28306, Test NMSE: 0.40504, NLL: 1.70206\n",
      "train task: 1, test tasks: [0, 1]\n",
      "Epoch 360/1500 || Train: Elbo: -349.1324 NMSE: 0.17376, Test NMSE: 0.63221, NLL: 2.62394\n",
      "Epoch 420/1500 || Train: Elbo: -335.2649 NMSE: 0.12468, Test NMSE: 0.66100, NLL: 3.38040\n",
      "Epoch 480/1500 || Train: Elbo: -322.5072 NMSE: 0.07650, Test NMSE: 0.68257, NLL: 2.59725\n",
      "Epoch 540/1500 || Train: Elbo: -310.5539 NMSE: 0.04030, Test NMSE: 0.61888, NLL: 2.31011\n",
      "Epoch 600/1500 || Train: Elbo: -336.6002 NMSE: 0.14326, Test NMSE: 0.68414, NLL: 2.78122\n",
      "train task: 3, test tasks: [0, 1, 3]\n",
      "Epoch 660/1500 || Train: Elbo: -628.0637 NMSE: 0.27310, Test NMSE: 1.10537, NLL: 3.65382\n",
      "Epoch 720/1500 || Train: Elbo: -571.2599 NMSE: 0.21730, Test NMSE: 1.40814, NLL: 3.69391\n",
      "Epoch 780/1500 || Train: Elbo: -1920.3015 NMSE: 1.24082, Test NMSE: 1.07542, NLL: 3.30075\n",
      "Epoch 840/1500 || Train: Elbo: -480.6422 NMSE: 0.12339, Test NMSE: 1.40250, NLL: 4.05786\n",
      "Epoch 900/1500 || Train: Elbo: -414.4040 NMSE: 0.06724, Test NMSE: 1.29130, NLL: 4.53699\n",
      "train task: 2, test tasks: [0, 1, 3, 2]\n",
      "Epoch 960/1500 || Train: Elbo: -349.8868 NMSE: 0.03880, Test NMSE: 0.91448, NLL: 4.04973\n",
      "Epoch 1020/1500 || Train: Elbo: -342.4898 NMSE: 0.03471, Test NMSE: 0.93001, NLL: 3.75903\n",
      "Epoch 1080/1500 || Train: Elbo: -333.2353 NMSE: 0.02580, Test NMSE: 0.86745, NLL: 4.20377\n",
      "Epoch 1140/1500 || Train: Elbo: -329.3631 NMSE: 0.02495, Test NMSE: 0.86483, NLL: 3.73223\n",
      "Epoch 1200/1500 || Train: Elbo: -325.2826 NMSE: 0.02061, Test NMSE: 0.83238, NLL: 3.96262\n",
      "train task: 4, test tasks: [0, 1, 3, 2, 4]\n",
      "Epoch 1260/1500 || Train: Elbo: -726.9264 NMSE: 0.22436, Test NMSE: 1.39852, NLL: 4.76422\n",
      "Epoch 1320/1500 || Train: Elbo: -437.9976 NMSE: 0.04898, Test NMSE: 1.32357, NLL: 4.11256\n",
      "Epoch 1380/1500 || Train: Elbo: -389.9869 NMSE: 0.02695, Test NMSE: 1.52637, NLL: 4.14172\n",
      "Epoch 1440/1500 || Train: Elbo: -365.3218 NMSE: 0.01790, Test NMSE: 1.61961, NLL: 4.39168\n",
      "Epoch 1499/1500 || Train: Elbo: -352.3745 NMSE: 0.01363, Test NMSE: 1.61804, NLL: 4.13748\n",
      "\n",
      "########## 2 ##########\n",
      "train task: 1, test tasks: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4843615aa9bc43bf948720a047aa4b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1500 || Train: Elbo: -583.3878 NMSE: 1.07419, Test NMSE: 1.04017, NLL: 3.86532\n",
      "Epoch 60/1500 || Train: Elbo: -434.6259 NMSE: 0.50251, Test NMSE: 0.55240, NLL: 2.53398\n",
      "Epoch 120/1500 || Train: Elbo: -346.6899 NMSE: 0.17513, Test NMSE: 0.19101, NLL: 1.99713\n",
      "Epoch 180/1500 || Train: Elbo: -333.8068 NMSE: 0.12292, Test NMSE: 0.15559, NLL: 1.73036\n",
      "Epoch 240/1500 || Train: Elbo: -360.7942 NMSE: 0.22048, Test NMSE: 0.09997, NLL: 1.82115\n",
      "Epoch 300/1500 || Train: Elbo: -313.1408 NMSE: 0.04918, Test NMSE: 0.02386, NLL: 1.00922\n",
      "train task: 4, test tasks: [1, 4]\n",
      "Epoch 360/1500 || Train: Elbo: -1506.8925 NMSE: 0.67849, Test NMSE: 0.98562, NLL: 4.25735\n",
      "Epoch 420/1500 || Train: Elbo: -748.8832 NMSE: 0.23281, Test NMSE: 0.89755, NLL: 4.15965\n",
      "Epoch 480/1500 || Train: Elbo: -503.4119 NMSE: 0.08925, Test NMSE: 0.84270, NLL: 3.58325\n",
      "Epoch 540/1500 || Train: Elbo: -385.6931 NMSE: 0.02678, Test NMSE: 0.87674, NLL: 3.36827\n",
      "Epoch 600/1500 || Train: Elbo: -575.5299 NMSE: 0.13959, Test NMSE: 0.85528, NLL: 3.99644\n",
      "train task: 3, test tasks: [1, 4, 3]\n",
      "Epoch 660/1500 || Train: Elbo: -351.1738 NMSE: 0.01893, Test NMSE: 0.59259, NLL: 3.66607\n",
      "Epoch 720/1500 || Train: Elbo: -333.8307 NMSE: 0.00974, Test NMSE: 0.63447, NLL: 3.31206\n",
      "Epoch 780/1500 || Train: Elbo: -361.5781 NMSE: 0.03953, Test NMSE: 0.50456, NLL: 3.34521\n",
      "Epoch 840/1500 || Train: Elbo: -333.5690 NMSE: 0.01232, Test NMSE: 0.64682, NLL: 3.54293\n",
      "Epoch 900/1500 || Train: Elbo: -329.9695 NMSE: 0.01245, Test NMSE: 0.55701, NLL: 3.88560\n",
      "train task: 2, test tasks: [1, 4, 3, 2]\n",
      "Epoch 960/1500 || Train: Elbo: -317.7761 NMSE: 0.01069, Test NMSE: 0.91793, NLL: 3.88341\n",
      "Epoch 1020/1500 || Train: Elbo: -314.7205 NMSE: 0.01010, Test NMSE: 0.90458, NLL: 3.85924\n",
      "Epoch 1080/1500 || Train: Elbo: -313.4561 NMSE: 0.00865, Test NMSE: 0.89783, NLL: 3.75412\n",
      "Epoch 1140/1500 || Train: Elbo: -378.8804 NMSE: 0.11318, Test NMSE: 1.24690, NLL: 3.79618\n",
      "Epoch 1200/1500 || Train: Elbo: -321.8716 NMSE: 0.02077, Test NMSE: 0.76658, NLL: 4.16235\n",
      "train task: 0, test tasks: [1, 4, 3, 2, 0]\n",
      "Epoch 1260/1500 || Train: Elbo: -326.4554 NMSE: 0.40340, Test NMSE: 1.46977, NLL: 4.73530\n",
      "Epoch 1320/1500 || Train: Elbo: -311.3306 NMSE: 0.18336, Test NMSE: 1.49962, NLL: 4.71878\n",
      "Epoch 1380/1500 || Train: Elbo: -304.6539 NMSE: 0.08193, Test NMSE: 1.48277, NLL: 4.57710\n",
      "Epoch 1440/1500 || Train: Elbo: -302.3679 NMSE: 0.06188, Test NMSE: 1.42423, NLL: 4.70509\n",
      "Epoch 1499/1500 || Train: Elbo: -304.6406 NMSE: 0.10976, Test NMSE: 1.41892, NLL: 4.89627\n",
      "\n",
      "########## 3 ##########\n",
      "train task: 1, test tasks: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992071f34b064e3991febbca50e545a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1500 || Train: Elbo: -563.7564 NMSE: 1.00751, Test NMSE: 1.03583, NLL: 3.59322\n",
      "Epoch 60/1500 || Train: Elbo: -431.2909 NMSE: 0.48840, Test NMSE: 0.59570, NLL: 4.38491\n",
      "Epoch 120/1500 || Train: Elbo: -527.1037 NMSE: 0.84136, Test NMSE: 0.88539, NLL: 3.96103\n",
      "Epoch 180/1500 || Train: Elbo: -354.8710 NMSE: 0.19348, Test NMSE: 0.16094, NLL: 1.81227\n",
      "Epoch 240/1500 || Train: Elbo: -329.4986 NMSE: 0.10022, Test NMSE: 0.08824, NLL: 1.04684\n",
      "Epoch 300/1500 || Train: Elbo: -341.8176 NMSE: 0.14906, Test NMSE: 0.13901, NLL: 1.60595\n",
      "train task: 4, test tasks: [1, 4]\n",
      "Epoch 360/1500 || Train: Elbo: -1206.0362 NMSE: 0.51239, Test NMSE: 1.01689, NLL: 4.71923\n",
      "Epoch 420/1500 || Train: Elbo: -1149.4402 NMSE: 0.47286, Test NMSE: 0.97068, NLL: 4.44561\n",
      "Epoch 480/1500 || Train: Elbo: -634.8441 NMSE: 0.16499, Test NMSE: 0.68170, NLL: 4.12532\n",
      "Epoch 540/1500 || Train: Elbo: -494.8028 NMSE: 0.08709, Test NMSE: 0.54544, NLL: 2.98307\n",
      "Epoch 600/1500 || Train: Elbo: -446.8491 NMSE: 0.06345, Test NMSE: 0.61345, NLL: 3.70150\n",
      "train task: 0, test tasks: [1, 4, 0]\n",
      "Epoch 660/1500 || Train: Elbo: -345.5903 NMSE: 0.57682, Test NMSE: 1.31072, NLL: 4.22816\n",
      "Epoch 720/1500 || Train: Elbo: -334.1703 NMSE: 0.42018, Test NMSE: 1.39370, NLL: 3.72360\n",
      "Epoch 780/1500 || Train: Elbo: -325.7161 NMSE: 0.32087, Test NMSE: 1.40898, NLL: 4.36032\n",
      "Epoch 840/1500 || Train: Elbo: -319.9273 NMSE: 0.24964, Test NMSE: 1.50198, NLL: 3.84684\n",
      "Epoch 900/1500 || Train: Elbo: -313.8905 NMSE: 0.16970, Test NMSE: 1.54812, NLL: 4.41947\n",
      "train task: 3, test tasks: [1, 4, 0, 3]\n",
      "Epoch 960/1500 || Train: Elbo: -402.8659 NMSE: 0.06530, Test NMSE: 0.61954, NLL: 2.92193\n",
      "Epoch 1020/1500 || Train: Elbo: -353.6159 NMSE: 0.02654, Test NMSE: 0.66256, NLL: 3.62353\n",
      "Epoch 1080/1500 || Train: Elbo: -348.0569 NMSE: 0.02675, Test NMSE: 0.65776, NLL: 3.90634\n",
      "Epoch 1140/1500 || Train: Elbo: -395.5780 NMSE: 0.07293, Test NMSE: 0.67208, NLL: 3.46418\n",
      "Epoch 1200/1500 || Train: Elbo: -340.0926 NMSE: 0.01955, Test NMSE: 0.60678, NLL: 3.08806\n",
      "train task: 2, test tasks: [1, 4, 0, 3, 2]\n",
      "Epoch 1260/1500 || Train: Elbo: -331.7298 NMSE: 0.02420, Test NMSE: 0.71252, NLL: 3.49813\n",
      "Epoch 1320/1500 || Train: Elbo: -322.8900 NMSE: 0.01703, Test NMSE: 0.69021, NLL: 3.64853\n",
      "Epoch 1380/1500 || Train: Elbo: -319.0607 NMSE: 0.01506, Test NMSE: 0.74758, NLL: 3.86622\n",
      "Epoch 1440/1500 || Train: Elbo: -315.9010 NMSE: 0.01407, Test NMSE: 0.75225, NLL: 3.91813\n",
      "Epoch 1499/1500 || Train: Elbo: -327.5491 NMSE: 0.02405, Test NMSE: 0.80576, NLL: 4.06633\n",
      "\n",
      "########## 4 ##########\n",
      "train task: 3, test tasks: [3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefc9f76a1a04e8ab8c0785f821703b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1500 || Train: Elbo: -1441.4174 NMSE: 1.01490, Test NMSE: 1.04229, NLL: 5.55970\n",
      "Epoch 60/1500 || Train: Elbo: -923.1426 NMSE: 0.55950, Test NMSE: 0.64145, NLL: 4.52191\n",
      "Epoch 120/1500 || Train: Elbo: -687.8027 NMSE: 0.33496, Test NMSE: 0.51804, NLL: 4.83831\n",
      "Epoch 180/1500 || Train: Elbo: -587.3781 NMSE: 0.23700, Test NMSE: 0.12673, NLL: 2.14255\n",
      "Epoch 240/1500 || Train: Elbo: -380.1192 NMSE: 0.04968, Test NMSE: 0.02449, NLL: 1.45138\n",
      "Epoch 300/1500 || Train: Elbo: -391.7322 NMSE: 0.05907, Test NMSE: 0.03878, NLL: 1.58881\n",
      "train task: 4, test tasks: [3, 4]\n",
      "Epoch 360/1500 || Train: Elbo: -1043.3315 NMSE: 0.42020, Test NMSE: 0.89361, NLL: 4.34718\n",
      "Epoch 420/1500 || Train: Elbo: -417.2541 NMSE: 0.04646, Test NMSE: 0.12362, NLL: 2.07887\n",
      "Epoch 480/1500 || Train: Elbo: -355.0490 NMSE: 0.01454, Test NMSE: 0.25280, NLL: 2.50133\n",
      "Epoch 540/1500 || Train: Elbo: -638.0749 NMSE: 0.17522, Test NMSE: 0.43842, NLL: 3.70818\n",
      "Epoch 600/1500 || Train: Elbo: -355.5464 NMSE: 0.01274, Test NMSE: 0.28571, NLL: 2.84781\n",
      "train task: 1, test tasks: [3, 4, 1]\n",
      "Epoch 660/1500 || Train: Elbo: -329.1521 NMSE: 0.06524, Test NMSE: 1.61470, NLL: 3.85264\n",
      "Epoch 720/1500 || Train: Elbo: -324.3897 NMSE: 0.05447, Test NMSE: 1.61390, NLL: 3.86982\n",
      "Epoch 780/1500 || Train: Elbo: -324.3185 NMSE: 0.05805, Test NMSE: 1.65416, NLL: 4.04160\n",
      "Epoch 840/1500 || Train: Elbo: -312.9609 NMSE: 0.02206, Test NMSE: 1.69119, NLL: 3.81277\n",
      "Epoch 900/1500 || Train: Elbo: -315.3790 NMSE: 0.03800, Test NMSE: 1.56150, NLL: 4.06020\n",
      "train task: 2, test tasks: [3, 4, 1, 2]\n",
      "Epoch 960/1500 || Train: Elbo: -324.0879 NMSE: 0.01833, Test NMSE: 0.90359, NLL: 3.79690\n",
      "Epoch 1020/1500 || Train: Elbo: -318.2095 NMSE: 0.01424, Test NMSE: 0.87101, NLL: 3.87422\n",
      "Epoch 1080/1500 || Train: Elbo: -336.0184 NMSE: 0.03687, Test NMSE: 1.18686, NLL: 4.15573\n",
      "Epoch 1140/1500 || Train: Elbo: -317.6231 NMSE: 0.01232, Test NMSE: 0.92244, NLL: 3.69210\n",
      "Epoch 1200/1500 || Train: Elbo: -318.5820 NMSE: 0.01897, Test NMSE: 0.90407, NLL: 3.72956\n",
      "train task: 0, test tasks: [3, 4, 1, 2, 0]\n",
      "Epoch 1260/1500 || Train: Elbo: -317.5535 NMSE: 0.28187, Test NMSE: 1.44472, NLL: 4.59328\n",
      "Epoch 1320/1500 || Train: Elbo: -305.4724 NMSE: 0.11915, Test NMSE: 1.62319, NLL: 4.62003\n",
      "Epoch 1380/1500 || Train: Elbo: -303.4137 NMSE: 0.08914, Test NMSE: 1.65170, NLL: 4.24497\n",
      "Epoch 1440/1500 || Train: Elbo: -301.5878 NMSE: 0.07434, Test NMSE: 1.64085, NLL: 4.32848\n",
      "Epoch 1499/1500 || Train: Elbo: -300.6588 NMSE: 0.07064, Test NMSE: 1.61204, NLL: 4.92687\n",
      "\n",
      "########## 5 ##########\n",
      "train task: 0, test tasks: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfe2c1d2cf0415ca94d94a3a8f34c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1500 || Train: Elbo: -360.5812 NMSE: 1.03732, Test NMSE: 1.03257, NLL: 2.04755\n",
      "Epoch 60/1500 || Train: Elbo: -338.3364 NMSE: 0.69428, Test NMSE: 0.81687, NLL: 2.67858\n",
      "Epoch 120/1500 || Train: Elbo: -310.1020 NMSE: 0.26286, Test NMSE: 0.48400, NLL: 2.04931\n",
      "Epoch 180/1500 || Train: Elbo: -316.6738 NMSE: 0.36596, Test NMSE: 0.72389, NLL: 1.47882\n",
      "Epoch 240/1500 || Train: Elbo: -317.8038 NMSE: 0.36577, Test NMSE: 0.31938, NLL: 1.48534\n",
      "Epoch 300/1500 || Train: Elbo: -301.9668 NMSE: 0.14067, Test NMSE: 0.26547, NLL: 1.48111\n",
      "train task: 4, test tasks: [0, 4]\n",
      "Epoch 360/1500 || Train: Elbo: -1182.0134 NMSE: 0.48814, Test NMSE: 0.53265, NLL: 3.15340\n",
      "Epoch 420/1500 || Train: Elbo: -705.2769 NMSE: 0.21360, Test NMSE: 0.75847, NLL: 3.96147\n",
      "Epoch 480/1500 || Train: Elbo: -597.4488 NMSE: 0.15062, Test NMSE: 0.84077, NLL: 3.97342\n",
      "Epoch 540/1500 || Train: Elbo: -562.1586 NMSE: 0.12447, Test NMSE: 0.78607, NLL: 3.61554\n",
      "Epoch 600/1500 || Train: Elbo: -434.5087 NMSE: 0.05331, Test NMSE: 0.89735, NLL: 2.75442\n",
      "train task: 2, test tasks: [0, 4, 2]\n",
      "Epoch 660/1500 || Train: Elbo: -348.7135 NMSE: 0.04856, Test NMSE: 1.39080, NLL: 4.31247\n",
      "Epoch 720/1500 || Train: Elbo: -339.9623 NMSE: 0.03962, Test NMSE: 1.40182, NLL: 4.04781\n",
      "Epoch 780/1500 || Train: Elbo: -328.8353 NMSE: 0.02753, Test NMSE: 1.37287, NLL: 4.19596\n",
      "Epoch 840/1500 || Train: Elbo: -363.5877 NMSE: 0.08696, Test NMSE: 1.23545, NLL: 3.67584\n",
      "Epoch 900/1500 || Train: Elbo: -320.9504 NMSE: 0.01943, Test NMSE: 1.40440, NLL: 4.22809\n",
      "train task: 1, test tasks: [0, 4, 2, 1]\n",
      "Epoch 960/1500 || Train: Elbo: -310.5842 NMSE: 0.02491, Test NMSE: 1.74876, NLL: 4.41744\n",
      "Epoch 1020/1500 || Train: Elbo: -308.1305 NMSE: 0.01984, Test NMSE: 1.76095, NLL: 4.51456\n",
      "Epoch 1080/1500 || Train: Elbo: -306.3997 NMSE: 0.01844, Test NMSE: 1.76953, NLL: 4.56378\n",
      "Epoch 1140/1500 || Train: Elbo: -307.2542 NMSE: 0.02278, Test NMSE: 1.85204, NLL: 4.38706\n",
      "Epoch 1200/1500 || Train: Elbo: -367.0391 NMSE: 0.24392, Test NMSE: 1.99799, NLL: 5.01920\n",
      "train task: 3, test tasks: [0, 4, 2, 1, 3]\n",
      "Epoch 1260/1500 || Train: Elbo: -696.9321 NMSE: 0.33448, Test NMSE: 1.01129, NLL: 4.29358\n",
      "Epoch 1320/1500 || Train: Elbo: -881.9058 NMSE: 0.47241, Test NMSE: 0.92956, NLL: 4.22913\n",
      "Epoch 1380/1500 || Train: Elbo: -475.7949 NMSE: 0.11236, Test NMSE: 0.83214, NLL: 3.67170\n",
      "Epoch 1440/1500 || Train: Elbo: -422.1201 NMSE: 0.07278, Test NMSE: 0.89749, NLL: 4.30276\n",
      "Epoch 1499/1500 || Train: Elbo: -388.7952 NMSE: 0.04824, Test NMSE: 0.93330, NLL: 4.37457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments = []\n",
    "for exp_num in range(args.start_replication, args.max_replication + 1):\n",
    "    print(\"#\"*10, exp_num, \"#\"*10)\n",
    "    results = replicate(args, device, exp_num)\n",
    "    experiments.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Tasks</th>\n",
       "      <th>NMSE</th>\n",
       "      <th>NLL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Tasks</td>\n",
       "      <td>0.174 +- 0.072</td>\n",
       "      <td>1.477 +- 0.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Tasks</td>\n",
       "      <td>0.667 +- 0.109</td>\n",
       "      <td>3.216 +- 0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Tasks</td>\n",
       "      <td>1.272 +- 0.186</td>\n",
       "      <td>4.226 +- 0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 Tasks</td>\n",
       "      <td>1.022 +- 0.249</td>\n",
       "      <td>3.992 +- 0.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 Tasks</td>\n",
       "      <td>1.278 +- 0.172</td>\n",
       "      <td>4.480 +- 0.183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Tasks            NMSE             NLL\n",
       "0  1 Tasks  0.174 +- 0.072  1.477 +- 0.122\n",
       "1  2 Tasks  0.667 +- 0.109  3.216 +- 0.263\n",
       "2  3 Tasks  1.272 +- 0.186  4.226 +- 0.118\n",
       "3  4 Tasks  1.022 +- 0.249  3.992 +- 0.314\n",
       "4  5 Tasks  1.278 +- 0.172  4.480 +- 0.183"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [\"nmse\", \"nll\"]\n",
    "n_tasks = num_tasks_dict[args.task]\n",
    "epochs = args.epochs\n",
    "ls = []\n",
    "\n",
    "indexes = [i*epochs for i in range(1, n_tasks)] + [n_tasks*epochs -1]\n",
    "for exp_num in range(len(experiments)):\n",
    "    for score in scores:\n",
    "        for i, idx in enumerate(indexes):\n",
    "            ls.append([f\"{i + 1} tasks\", score, experiments[exp_num][f\"{score}_test\"][experiments[exp_num][f\"idx_test\"].index(idx)]])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(ls, columns=['# Tasks', 'score', 'value'])\n",
    "results = []\n",
    "for nof_task in range(1, n_tasks + 1):\n",
    "    mse = df[df.score == \"nmse\"]\n",
    "    nll = df[df.score == \"nll\"]\n",
    "    mse = mse[mse[\"# Tasks\"] == f\"{nof_task} tasks\"]\n",
    "    nll = nll[nll[\"# Tasks\"] == f\"{nof_task} tasks\"]\n",
    "    results.append([f\"{nof_task} Tasks\", f\"{mse.value.mean():.3f} +- {sem(mse.value):.3f}\", f\"{nll.value.mean():.3f} +- {sem(nll.value):.3f}\"])\n",
    "\n",
    "results = pd.DataFrame(results, columns=[\"# Tasks\", 'NMSE', 'NLL'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMSE: 0.883 +- 0.110 \n",
      "NLL: 3.478 +- 0.238 \n"
     ]
    }
   ],
   "source": [
    "print(f'NMSE: {df[df.score == \"nmse\"].value.mean():.3f} +- {sem(df[df.score == \"nmse\"].value):.3f} ')\n",
    "print(f'NLL: {df[df.score == \"nll\"].value.mean():.3f} +- {sem(df[df.score == \"nll\"].value):.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
